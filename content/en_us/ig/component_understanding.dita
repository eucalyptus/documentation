<?xml version="1.0" encoding="UTF-8"?>
<!--This work by Eucalyptus Systems is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. See the accompanying LICENSE file for more information.-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="component_understanding">
 <title>Understanding Component Placement</title>
 <shortdesc/>
 <prolog>
  <metadata>
   <keywords>
    <indexterm>components 
    	<indexterm>cloud</indexterm>
    	<indexterm>cluster</indexterm>
    	<indexterm>node</indexterm>
    </indexterm>
   </keywords>
  </metadata>
 </prolog>

 <conbody>
  <p>A Eucalyptus deployment is a set of cloud services (Cloud Controller and Walrus) and one
   or more clusters, each of which contains a Cluster Controller, a Storage Controller, an optional VMware
   Broker (located with the Cluster Controller), and one or more Node Controllers. </p>
  <image href="../shared/images/euca-arch-extras-tiers.png"/>
 	
 	<section>
 		<title>Cloud Components</title>
 		<p>The main decision for cloud components is whether to install the Cloud Controller (CLC)
 			and Walrus on the same server. If they are on the same server, they
 			operate as separate web services within a single Java environment,
 			and they use a fast-path for inter-service communication. If they
 			are not on the same server, they use SOAP and REST to work
 			together.</p>
 		<p>However, when installed on the same server, the CLC and Walrus must
 			share a common memory footprint, both managed by the Java memory
 			manager. Walrus self-tunes its performance based on the memory
 			pressure it perceives and runs faster with more memory. So, while
 			separating the CLC and Walrus decreases the efficiency of the
 			messaging between the two, it often increases the responsiveness of
 			the overall Eucalyptus system when Walrus is given a large memory
 			footprint.</p>
 		<p>Sometimes the key factor for cloud components is not performance, but
 			server cost and data center configuration. If you only have one
 			server available for the cloud, then you have to install the
 			components on the same server.</p>
 		<p>The CLC and Walrus components are not designed to be separated by
 			wide-area, common carrier networks. They use aggressive time-outs to
 			maintain system responsiveness so separating them over a
 			long-latency, lossy network link will not work.</p>
 		<p>The CLC and Walrus communicate with Eucalyptus clients independently.
 			End-users typically interact with Eucalyptus through a client
 			interface. They can use either our provided euca2ools Linux command
 			line client tools, or the Eucalyptus AWS-compatible API, or a
 			third-party client that is compatible with Eucalyptus. In all cases,
 			the end-user client must be able to send messages via TCP/IP to the
 			machine on which the CLC is deployed.</p>
 		<p>In addition, the CLC must have TCP/IP connectivity to all other
 			Eucalyptus components except for node controllers (NCs), which may
 			reside on their own private networks. In addition, NC servers must
 			be able to send messages to the Walrus server because images are
 			downloaded by the NC using the Walrus URL. That is, the CLC does not
 			need to be able to route network traffic directly to the NCs but
 			Walrus does for the purposes of image delivery.</p>
 	</section>
 	
 	<section>
 		<title>Cluster Components</title>
 		<p>The Eucalyptus components deployed in the cluster level of a
 			Eucalyptus deployment are the Cluster Controller (CC), Storage
 			Controller (SC), and VMware Broker.</p>
 		<note type="tip">The VMware Broker is available by subscription only.
 			You do not need the VMware Broker unless you are using VMware
 			hypervisor.</note>
 		<p>You can install all cluster components on a single machine, or you
 			can distribute them on different machines. The choice of one or
 			multiple machines is dictated by the demands of user workload in
 			terms of external network utilization (CC) and EBS volume access
 			(SC). </p>
 		
 		<p>Things to consider for CC placement:</p>
 		<ul>
 			<li>If you plan to use elastic IPs and security groups, the CC
 				physical machine becomes a software IP gateway between VM
 				instances and the public network. Because of this software
 				routing function, the physical server on which the CC is
 				deployed should have fast, dedicated network access to both the
 				NC network, and the public network. </li>
 			<li>If you don’t plan to use elastic IPs or security groups, the CC
 				physical machine will not act as a software gateway. Network
 				traffic will be limited to small control messages. </li>
 			<li>In all cases, place the CC on a machine that has TCP/IP
 				connectivity to the Eucalyptus front end servers and the NC
 				servers in its cluster.</li>
 		</ul>
 		
 		
 			<p>Things to consider for SC placement:</p>
 		<ul>
 			<li>The machine on which the SC is deployed must always have TCP/IP
 				connectivity to the CLC. If you are a subscriber and use one of
 				Eucalyptus’ provided SAN integration drivers, the SC must also
 				have TCP/IP connectivity to the chosen SAN device. In this case,
 				the SC only sends control messages to the SAN. </li>
 			<li>If you do not configure a SAN, the SC requires only TCP/IP
 				connectivity to the NCs in the cluster. The SC will use this
 				TCP/IP connectivity to provide the NCs network access to the
 				dynamic block volumes residing on the SC’s storage. SC storage
 				should consist of a fast, reliable disk pool (either local
 				file-system or block-attached storage) so that the SC can create
 				and maintain volumes for the NCs. The capacity of the disk pool
 				should be sufficient to provide the NCs with enough space to
 				accommodate all dynamic block volumes requests from
 				end-users</li>
 		</ul>
 	</section>
 	
 	<section>
 		<title>Node Components</title>
 		<p>The Node Controllers are the components that comprise the Eucalyptus
 			back-end. All NCs must have network connectivity to whatever hosts their EBS
 			volumes. This host is either a SAN or the SC.</p>
 	</section>

 </conbody>
</concept>
